{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbaa6966",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31affc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268e37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6cf9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SATAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2987fe4bca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180ebcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('/C:/Users/dell/Desktop/Python/Tableau_DataSet.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92babf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff6dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2016-138688|12-06-2016|16-06-2016|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef67e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439dd7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Row ID',\n",
       " 'Order ID',\n",
       " 'Order Date',\n",
       " 'Ship Date',\n",
       " 'Ship Mode',\n",
       " 'Customer ID',\n",
       " 'Customer Name',\n",
       " 'Segment',\n",
       " 'Country',\n",
       " 'City',\n",
       " 'State',\n",
       " 'Postal Code',\n",
       " 'Region',\n",
       " 'Product ID',\n",
       " 'Category',\n",
       " 'Sub-Category',\n",
       " 'Product Name',\n",
       " 'Sales',\n",
       " 'Quantity',\n",
       " 'Discount',\n",
       " 'Profit']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3138cd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Row ID=1, Order ID='CA-2016-152156', Order Date='08-11-2016', Ship Date='11-11-2016', Ship Mode='Second Class', Customer ID='CG-12520', Customer Name='Claire Gute', Segment='Consumer', Country='United States', City='Henderson', State='Kentucky', Postal Code=42420, Region='South', Product ID='FUR-BO-10001798', Category='Furniture', Sub-Category='Bookcases', Product Name='Bush Somerset Collection Bookcase', Sales='261.96', Quantity='2', Discount='0', Profit=41.9136),\n",
       " Row(Row ID=2, Order ID='CA-2016-152156', Order Date='08-11-2016', Ship Date='11-11-2016', Ship Mode='Second Class', Customer ID='CG-12520', Customer Name='Claire Gute', Segment='Consumer', Country='United States', City='Henderson', State='Kentucky', Postal Code=42420, Region='South', Product ID='FUR-CH-10000454', Category='Furniture', Sub-Category='Chairs', Product Name='Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back', Sales='731.94', Quantity='3', Discount='0', Profit=219.582),\n",
       " Row(Row ID=3, Order ID='CA-2016-138688', Order Date='12-06-2016', Ship Date='16-06-2016', Ship Mode='Second Class', Customer ID='DV-13045', Customer Name='Darrin Van Huff', Segment='Corporate', Country='United States', City='Los Angeles', State='California', Postal Code=90036, Region='West', Product ID='OFF-LA-10000240', Category='Office Supplies', Sub-Category='Labels', Product Name='Self-Adhesive Address Labels for Typewriters by Universal', Sales='14.62', Quantity='2', Discount='0', Profit=6.8714),\n",
       " Row(Row ID=4, Order ID='US-2015-108966', Order Date='11-10-2015', Ship Date='18-10-2015', Ship Mode='Standard Class', Customer ID='SO-20335', Customer Name=\"Sean O'Donnell\", Segment='Consumer', Country='United States', City='Fort Lauderdale', State='Florida', Postal Code=33311, Region='South', Product ID='FUR-TA-10000577', Category='Furniture', Sub-Category='Tables', Product Name='Bretford CR4500 Series Slim Rectangular Table', Sales='957.5775', Quantity='5', Discount='0.45', Profit=-383.031),\n",
       " Row(Row ID=5, Order ID='US-2015-108966', Order Date='11-10-2015', Ship Date='18-10-2015', Ship Mode='Standard Class', Customer ID='SO-20335', Customer Name=\"Sean O'Donnell\", Segment='Consumer', Country='United States', City='Fort Lauderdale', State='Florida', Postal Code=33311, Region='South', Product ID='OFF-ST-10000760', Category='Office Supplies', Sub-Category='Storage', Product Name=\"Eldon Fold 'N Roll Cart System\", Sales='22.368', Quantity='2', Discount='0.2', Profit=2.5164)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b593e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     Customer Name|\n",
      "+------------------+\n",
      "|       Claire Gute|\n",
      "|       Claire Gute|\n",
      "|   Darrin Van Huff|\n",
      "|    Sean O'Donnell|\n",
      "|    Sean O'Donnell|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|   Brosina Hoffman|\n",
      "|      Andrew Allen|\n",
      "|      Irene Maddox|\n",
      "|     Harold Pawlan|\n",
      "|     Harold Pawlan|\n",
      "|         Pete Kriz|\n",
      "|   Alejandro Grove|\n",
      "|Zuschuss Donatelli|\n",
      "|Zuschuss Donatelli|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Customer Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a400ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|Customer ID|     Customer Name|\n",
      "+-----------+------------------+\n",
      "|   CG-12520|       Claire Gute|\n",
      "|   CG-12520|       Claire Gute|\n",
      "|   DV-13045|   Darrin Van Huff|\n",
      "|   SO-20335|    Sean O'Donnell|\n",
      "|   SO-20335|    Sean O'Donnell|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   BH-11710|   Brosina Hoffman|\n",
      "|   AA-10480|      Andrew Allen|\n",
      "|   IM-15070|      Irene Maddox|\n",
      "|   HP-14815|     Harold Pawlan|\n",
      "|   HP-14815|     Harold Pawlan|\n",
      "|   PK-19075|         Pete Kriz|\n",
      "|   AG-10270|   Alejandro Grove|\n",
      "|   ZD-21925|Zuschuss Donatelli|\n",
      "|   ZD-21925|Zuschuss Donatelli|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Customer ID', 'Customer Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2635d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Row ID', 'int'),\n",
       " ('Order ID', 'string'),\n",
       " ('Order Date', 'string'),\n",
       " ('Ship Date', 'string'),\n",
       " ('Ship Mode', 'string'),\n",
       " ('Customer ID', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('Segment', 'string'),\n",
       " ('Country', 'string'),\n",
       " ('City', 'string'),\n",
       " ('State', 'string'),\n",
       " ('Postal Code', 'int'),\n",
       " ('Region', 'string'),\n",
       " ('Product ID', 'string'),\n",
       " ('Category', 'string'),\n",
       " ('Sub-Category', 'string'),\n",
       " ('Product Name', 'string'),\n",
       " ('Sales', 'string'),\n",
       " ('Quantity', 'string'),\n",
       " ('Discount', 'string'),\n",
       " ('Profit', 'double')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1813d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------+----------+----------+--------------+-----------+------------------+-----------+-------------+--------+-------+------------------+-------+---------------+----------+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|            Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|     Customer Name|    Segment|      Country|    City|  State|       Postal Code| Region|     Product ID|  Category|Sub-Category|        Product Name|             Sales|          Quantity|          Discount|            Profit|\n",
      "+-------+------------------+--------------+----------+----------+--------------+-----------+------------------+-----------+-------------+--------+-------+------------------+-------+---------------+----------+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              9994|          9994|      9994|      9994|          9994|       9994|              9994|       9994|         9994|    9994|   9994|              9994|   9994|           9994|      9994|        9994|                9994|              9994|              9994|              9994|              9994|\n",
      "|   mean|            4997.5|          null|      null|      null|          null|       null|              null|       null|         null|    null|   null|  55190.3794276566|   null|           null|      null|        null|                null|234.41818199917006| 5.828590535392018|0.3155949113492862|28.587912967780834|\n",
      "| stddev|2885.1636290974325|          null|      null|      null|          null|       null|              null|       null|         null|    null|   null|32063.693350364487|   null|           null|      null|        null|                null| 631.7890112674363|25.520975563736403| 3.314008629792499| 234.3891156047269|\n",
      "|    min|                 1|CA-2014-100006|01-01-2017|01-01-2015|   First Class|   AA-10315|     Aaron Bergman|   Consumer|United States|Aberdeen|Alabama|              1040|Central|FUR-BO-10000112| Furniture| Accessories|\"\"\"While you Were...|          10/Pack\"|      1040 sheets\"|           30/Box\"|         -6599.978|\n",
      "|    max|              9994|US-2017-169551|31-12-2016|31-12-2017|Standard Class|   ZD-21925|Zuschuss Donatelli|Home Office|United States|    Yuma|Wyoming|             99301|   West|TEC-PH-10004977|Technology|      Tables|netTALK DUO VoIP ...|            999.98|            98.352|            98.352|          8399.976|\n",
      "+-------+------------------+--------------+----------+----------+--------------+-----------+------------------+-----------+-------------+--------+-------+------------------+-------+---------------+----------+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9186e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('New Discount', df_pyspark['Discount'] + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b891a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|       New Discount|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+-------------------+\n",
      "|     1|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|                0.1|\n",
      "|     2|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|                0.1|\n",
      "|     3|CA-2016-138688|12-06-2016|16-06-2016|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|                0.1|\n",
      "|     4|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|               0.55|\n",
      "|     5|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|0.30000000000000004|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "202c7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('New Discount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d1f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2016-138688|12-06-2016|16-06-2016|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28f8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumnRenamed('Order ID', 'Order Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e947d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|  Order Number|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2016-152156|08-11-2016|11-11-2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2016-138688|12-06-2016|16-06-2016|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2015-108966|11-10-2015|18-10-2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d274c",
   "metadata": {},
   "source": [
    "## Handling the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c6184de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_test = spark.read.csv('test.csv', header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99589610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a38cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  56|      null|  null|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d105814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "|Vaibhav|  23|         3|\n",
      "|  Ankit|  20|         9|\n",
      "|Bipasha|  32|      null|\n",
      "| Chetan|null|         8|\n",
      "|Diwakar|  21|         5|\n",
      "|   null|  34|         8|\n",
      "|Supriya|  27|      null|\n",
      "|  Raina|  29|         2|\n",
      "|  Neetu|  40|         3|\n",
      "|  Aryan|  32|         5|\n",
      "|   null|  34|        10|\n",
      "|   null|  56|      null|\n",
      "|   Ritu|  31|         6|\n",
      "|  Priya|  25|         5|\n",
      "| Yogita|  28|         3|\n",
      "| Snehal|  24|         6|\n",
      "|  Rohan|  27|         5|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.drop('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ac9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2dc45d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b92bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  56|      null|  null|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eca0ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f91d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(thresh = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8b804f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(subset = 'Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea218320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.drop(subset = 'Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3e1242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|      Vaibhav|  23|         3| 10000|\n",
      "|        Ankit|  20|         9| 20000|\n",
      "|      Bipasha|  32|      null| 34000|\n",
      "|       Chetan|null|         8| 54440|\n",
      "|      Diwakar|  21|         5| 23000|\n",
      "|Not Available|  34|         8| 54000|\n",
      "|      Supriya|  27|      null| 32000|\n",
      "|        Raina|  29|         2|  null|\n",
      "|        Neetu|  40|         3| 50000|\n",
      "|        Aryan|  32|         5| 45000|\n",
      "|Not Available|  34|        10| 38000|\n",
      "|Not Available|  56|      null|  null|\n",
      "|         Ritu|  31|         6| 25000|\n",
      "|        Priya|  25|         5| 24000|\n",
      "|       Yogita|  28|         3| 42000|\n",
      "|       Snehal|  24|         6| 26000|\n",
      "|        Rohan|  27|         5| 52000|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.fill('Not Available').show()\n",
    "\n",
    "# age,experience,salary are int type so not getting filled with string 'not available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9734d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Bipasha| 32|         0| 34000|\n",
      "| Chetan|  0|         8| 54440|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|   null| 34|         8| 54000|\n",
      "|Supriya| 27|         0| 32000|\n",
      "|  Raina| 29|         2|     0|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   null| 34|        10| 38000|\n",
      "|   null| 56|         0|     0|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.fill(0, subset = ['Salary', 'Experience', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e5f6016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------------------+------------------+\n",
      "|summary|  Name|              Age|        Experience|            Salary|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "|  count|    14|               16|                14|                15|\n",
      "|   mean|  null|          30.1875| 5.571428571428571|           35296.0|\n",
      "| stddev|  null|8.681157756889343|2.4405007592795287|13927.360943737434|\n",
      "|    min| Ankit|               20|                 2|             10000|\n",
      "|    max|Yogita|               56|                10|             54440|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "956f2e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Bipasha| 32|      null| 34000|\n",
      "| Chetan| 31|         8| 54440|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|   null| 34|         8| 54000|\n",
      "|Supriya| 27|      null| 32000|\n",
      "|  Raina| 29|         2|  null|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   null| 34|        10| 38000|\n",
      "|   null| 56|      null|  null|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.fill(31, subset = 'Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d29b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|         5| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|         5| 32000|\n",
      "|  Raina|  29|         2|  null|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  56|         5|  null|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.fill(5, subset = 'Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d150f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Vaibhav|  23|         3| 10000|\n",
      "|  Ankit|  20|         9| 20000|\n",
      "|Bipasha|  32|      null| 34000|\n",
      "| Chetan|null|         8| 54440|\n",
      "|Diwakar|  21|         5| 23000|\n",
      "|   null|  34|         8| 54000|\n",
      "|Supriya|  27|      null| 32000|\n",
      "|  Raina|  29|         2| 36044|\n",
      "|  Neetu|  40|         3| 50000|\n",
      "|  Aryan|  32|         5| 45000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  56|      null| 36044|\n",
      "|   Ritu|  31|         6| 25000|\n",
      "|  Priya|  25|         5| 24000|\n",
      "| Yogita|  28|         3| 42000|\n",
      "| Snehal|  24|         6| 26000|\n",
      "|  Rohan|  27|         5| 52000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.na.fill(36044, subset = 'Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dd427ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTER FUNCTION\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "                  inputCols = ['Age', 'Experience', 'Salary'],\n",
    "                  outputCols = [f'{c}_imputed' for c in ['Age', 'Experience', 'Salary']]\n",
    "                 ).setStrategy('mean')\n",
    "\n",
    "# put median in .setStrategy('median') to replace the null values with median values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "318890b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|Vaibhav|  23|         3| 10000|         23|                 3|         10000|\n",
      "|  Ankit|  20|         9| 20000|         20|                 9|         20000|\n",
      "|Bipasha|  32|      null| 34000|         32|                 5|         34000|\n",
      "| Chetan|null|         8| 54440|         30|                 8|         54440|\n",
      "|Diwakar|  21|         5| 23000|         21|                 5|         23000|\n",
      "|   null|  34|         8| 54000|         34|                 8|         54000|\n",
      "|Supriya|  27|      null| 32000|         27|                 5|         32000|\n",
      "|  Raina|  29|         2|  null|         29|                 2|         35296|\n",
      "|  Neetu|  40|         3| 50000|         40|                 3|         50000|\n",
      "|  Aryan|  32|         5| 45000|         32|                 5|         45000|\n",
      "|   null|  34|        10| 38000|         34|                10|         38000|\n",
      "|   null|  56|      null|  null|         56|                 5|         35296|\n",
      "|   Ritu|  31|         6| 25000|         31|                 6|         25000|\n",
      "|  Priya|  25|         5| 24000|         25|                 5|         24000|\n",
      "| Yogita|  28|         3| 42000|         28|                 3|         42000|\n",
      "| Snehal|  24|         6| 26000|         24|                 6|         26000|\n",
      "|  Rohan|  27|         5| 52000|         27|                 5|         52000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark_test).transform(df_pyspark_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef30d0f",
   "metadata": {},
   "source": [
    "## Filtering the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34d22e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_test = df_pyspark_test.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "404bc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c14e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf70c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "| Neetu| 40|         3| 50000|\n",
      "| Aryan| 32|         5| 45000|\n",
      "|Yogita| 28|         3| 42000|\n",
      "| Rohan| 27|         5| 52000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter('Salary >= 30000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "621c8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Vaibhav| 23|\n",
      "|  Ankit| 20|\n",
      "|Diwakar| 21|\n",
      "|   Ritu| 31|\n",
      "|  Priya| 25|\n",
      "| Snehal| 24|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter('Salary <= 30000').select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90e94d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter(df_pyspark_test['Salary'] <= 30000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c43634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter((df_pyspark_test['Salary'] >= 20000) & \n",
    "                       (df_pyspark_test[\"Age\"] <= 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "240dc0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Ankit| 20|         9| 20000|\n",
      "|Diwakar| 21|         5| 23000|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "|  Priya| 25|         5| 24000|\n",
      "| Yogita| 28|         3| 42000|\n",
      "| Snehal| 24|         6| 26000|\n",
      "|  Rohan| 27|         5| 52000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter((df_pyspark_test['Salary'] >= 20000) | \n",
    "                       (df_pyspark_test[\"Age\"] <= 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcbf9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Vaibhav| 23|         3| 10000|\n",
      "|  Neetu| 40|         3| 50000|\n",
      "|  Aryan| 32|         5| 45000|\n",
      "|   Ritu| 31|         6| 25000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test.filter(~((df_pyspark_test['Salary'] >= 20000) & \n",
    "                       (df_pyspark_test[\"Age\"] <= 30))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e03985",
   "metadata": {},
   "source": [
    "## Groupby & Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73a83437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_test1 = spark.read.csv('test1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd7809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dde8cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+\n",
      "|  Name|  Department|Salary|\n",
      "+------+------------+------+\n",
      "| Arjun|Data Science| 10000|\n",
      "| Richa|         IOT|  5000|\n",
      "| Soham|Data Science|  4500|\n",
      "| Arjun|    Big Data| 12000|\n",
      "| Arjun|         IOT| 14000|\n",
      "| Soham|    Big Data|  9000|\n",
      "|Neeraj|         IOT|  7000|\n",
      "| Richa|Data Science|  2300|\n",
      "| Parth|    Big Data|  5000|\n",
      "| Parth|         IOT| 11000|\n",
      "|Bhavya|Data Science|  6500|\n",
      "+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "987480ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  Name|sum(Salary)|\n",
      "+------+-----------+\n",
      "| Soham|      13500|\n",
      "| Richa|       7300|\n",
      "|Neeraj|       7000|\n",
      "| Parth|      16000|\n",
      "|Bhavya|       6500|\n",
      "| Arjun|      36000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.groupBy('Name').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19de9071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  Department|      avg(Salary)|\n",
      "+------------+-----------------+\n",
      "|         IOT|           9250.0|\n",
      "|    Big Data|8666.666666666666|\n",
      "|Data Science|           5825.0|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.groupBy('Department').avg('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "822b6bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|max(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      14000|\n",
      "|    Big Data|      12000|\n",
      "|Data Science|      10000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.groupBy('Department').max('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e26e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|min(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|       5000|\n",
      "|    Big Data|       5000|\n",
      "|Data Science|       2300|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.groupBy('Department').min('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e659f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    4|\n",
      "|    Big Data|    3|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f14f792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      86300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.agg({'Salary' : 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74ff0fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(Salary)|\n",
      "+-----------------+\n",
      "|7845.454545454545|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.agg({'Salary' : 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ca09c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Salary)|\n",
      "+-----------+\n",
      "|      14000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.agg({'Salary' : 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c9bbf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(Salary)|\n",
      "+-----------+\n",
      "|       2300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_test1.agg({'Salary' : 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3cabd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
